# Actor Critic


We view actor critic-algorithms as stochastic gradient algorithms on the parameter
space of the actor. 

**Critic-only** Aproximate Dynamic programming!

**Actor-only**   directly
estimated by simulation, and the parameters are updated in a direction of
improvement.  

*Drawback* of such methods is that the
gradient estimators may have a large variance. 

Famous example is the Reinforce algorithm: As a stochastic gradient method, REINFORCE has good theoretical convergence properties.
 However, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning.

 Then, there was an update for this: *REINFORCE with Baseline*


 Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actorâ€“critic method because its state-value function is used only as a baseline, not as a critic.

 





