# Hedging European option: by solving stochastic LQR problem using Actor-Critic

European options are contracts that give the owner the right, but not the obligation, to buy or sell the underlying security at a specific price, known as the strike price, on the option's expiration date. A European call option gives the owner the right to purchase the underlying security, while a European put option gives the owner the right to sell the underlying security.


Black-Scholes model:

$dS_t = S_t(\mu dt+\sigma dB_t)$, $S_0=s$, where $B$ is a Brownian process.

"Discreterized version"

$S_{n+1} =  S_n + S_n\mu \delta_t + S_n \sigma Z \sqrt{\delta_t}$, where $Z$ us standard normal variable.

**What's game-> Hedging without model assumption.**

The key idea behind the model is to hedge the option by buying and selling the underlying asset in just the right way and, as a consequence, to eliminate risk. This type of hedging is called "continuously revised delta hedging" and is the basis of more complicated hedging strategies such as those engaged in by investment banks and hedge funds.

$V^d(t) =  a_t S^0_t + d_t S_t$, $V(0) = option\_price$ where $a_t S^0_t$ is bank account value at time $t$, and $d_t$ is number of stock (policy).

We will choose $d_t$ such that it optimize the difference between true option price (or market price of the option) at each time $t$. 

In Black-Scholes model, the theorical optimize value for $d$ is called Delta-hedging and defined by

$d_t= N(d_1(t))$ where

$N(x) = Standard\_normal\_CDF(x)$

$d1(t) = \frac{1}{\sigma \sqrt{T-t}} (ln(\frac{S_t}{K})+(r+0.5\sigma^2)(T-t))$.

The option price we use in this game (will use in contructing reward) is:

$c(t,S_t) = N(d_1(t))S_t + N(d_2(t)) K e^{-r(T-t)}$, where

$d_2(t) = d_1(t) - \sigma \sqrt{T-t}$.

The optimal cotrol problem:

$min_d\sum_{t_i \leq T} e^{-rt_i}(V^d(t_i) - C(t_i,S_{t_i}))^2$

Stochastic LQR type !

We will try to solve this hedging problem using RL without knowledge about model, economic view,...

# Actor Critic


We view actor critic-algorithms as stochastic gradient algorithms on the parameter
space of the actor. 

**Critic-only** Aproximate Dynamic programming!

**Actor-only**   directly
estimated by simulation, and the parameters are updated in a direction of
improvement.  

*Drawback* of such methods is that the
gradient estimators may have a large variance. 

Famous example is the Reinforce algorithm: As a stochastic gradient method, REINFORCE has good theoretical convergence properties.
 However, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning.

 Then, there was an update for this: *REINFORCE with Baseline*

*What is the problem of*  *REINFORCE with Baseline* ?

 Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actorâ€“critic method because its state-value function is used only as a baseline, not as a critic.


[Good reference for Actor-Critic](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf)
