# Hedging European option: by solving stochastic LQR problem using Actor-Critic

European options are contracts that give the owner the right, but not the obligation, to buy or sell the underlying security at a specific price, known as the strike price, on the option's expiration date. A European call option gives the owner the right to purchase the underlying security, while a European put option gives the owner the right to sell the underlying security.


Black-Scholes model:

$dS_t = S_t(\mu dt+\sigma dB_t)$, $S_0=s$, where $B$ is a Brownian process.

"Discretized version"

$S_{n+1} =  S_n + S_n\mu \delta_t + S_n \sigma Z \sqrt{\delta_t}$, where $Z$ us standard normal variable.

**What's game-> Hedging without model assumption.**

The key idea behind the model is to hedge the option by buying and selling the underlying asset in just the right way and, as a consequence, to eliminate risk. This type of hedging is called "continuously revised delta hedging" and is the basis of more complicated hedging strategies such as those engaged in by investment banks and hedge funds.

$V^d(t) =  a_t S^0_t + d_t S_t$, $V^d(0) = option\_price$ where $a_t S^0_t$ is bank account value at time $t$, and $d_t$ is number of stock (policy).

We will choose $d_t$ such that it optimize the difference between true option price (or market price of the option) at each time $t$. 

In Black-Scholes model, the theorical optimize value for $d$ is called Delta-hedging and defined by

$d_t= N(d_1(t))$ where

$N(x) = Standard-normal-CDF(x)$

$d1(t) = \frac{1}{\sigma \sqrt{T-t}} (ln(\frac{S_t}{K})+(r+0.5\sigma^2)(T-t))$.

The option price we use in this game (will use in contructing reward) is:

$c(t,S_t) = N(d_1(t))S_t + N(d_2(t)) K e^{-r(T-t)}$, where

$d_2(t) = d_1(t) - \sigma \sqrt{T-t}$.

The optimal cotrol problem:

$min_d\sum_{t_i \leq T} e^{-rt_i}(V^d(t_i) - C(t_i,S_{t_i}))^2$

Stochastic LQR type !

We will try to solve this hedging problem using RL without knowledge about model, economic view,...

# Actor Critic


We view actor critic-algorithms as stochastic gradient algorithms on the parameter
space of the actor. 

**Critic-only** Aproximate Dynamic programming!

**Actor-only**   directly
estimated by simulation, and the parameters are updated in a direction of
improvement.  

*Drawback* of such methods (Actor-only) is that the
gradient estimators may have a large variance. 

Famous example is the Reinforce algorithm: As a stochastic gradient method, REINFORCE has good theoretical convergence properties.
 However, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning.

 $\nabla	J(\theta) = E[R(\tau)\nabla \log(\pi^{\theta}(s,a)) ]$

 Then, there was an update for this: *REINFORCE with Baseline*

 $\nabla	J(\theta) = E_{\tau \sim \pi^{\theta}}[(R(\tau)-b)\nabla \log(\pi^{\theta}(\tau)) ]$

 The estimation (using MC) for  $\nabla	J(\theta)$ is still unbias but we control variate through $b$.


*What is the problem of*  *REINFORCE with Baseline* ? Finding a good baseline is another challenge !

### Policy gradient theorem
(Average return) 
$\nabla	J(\theta) = \sum_s d^{\pi (\theta)}(s) E_{a \sim \pi^{\theta}}\left((Q^{\theta}(s,a) - b(s)) \nabla \log(\pi^{\theta}(s,a)) \right)$

(Episodic)
$\nabla	J(\theta) =  E_{a \sim \pi^{\theta}}\left((Q^{\theta}(s,a) - b(s)) \nabla \log(\pi^{\theta}(s,a)) \right)$

*If $Q$ is approximated by MC and $b=0$ then this will be Reinforce. Thus, as the policy changes, a new gradient is estimated independently of past estimates.*

We can choose $b$ as aproximate value function (we can update this choice), but at this time $b$ is depend on $policy$ paramters $\rightarrow$ unbias ?

[Good reference for Actor-Critic 1](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf)


[Good reference for policy gradient](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#policy-gradient-theorem)



* Policy gradient (above) is on-policy!

* But using the trick as importance sampling, we can have off-policy version !



